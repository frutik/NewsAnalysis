{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from itertools import islice, chain\n",
    "import string\n",
    "import sys  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "import news_config as config\n",
    "from NewsMongoMiner import NewsMongoHelper\n",
    "\n",
    "#set encoding for Cyrillic\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adds cyrillic to plot figures\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "matplotlib.rc('font', family='DejaVu Sans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16379"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem = Mystem()\n",
    "\n",
    "mongo_helper = NewsMongoHelper()\n",
    "news = mongo_helper.get_news(config.pravda_collection)\n",
    "articles = [item['text'] for item in news]\n",
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = [u'', u' ', u'-', '\\n', u'–', u'это', u'еще', u'него', u'сказать', u'а', u'ж', u'нее', u'со', u'без', u'же', u'ней', \n",
    "      u'совсем', u'более', u'жизнь', u'нельзя', u'так', u'больше', u'за', u'нет', \n",
    "      u'такой', u'будет', u'зачем', u'ни', u'там', u'будто', u'здесь', u'нибудь', u'тебя', \n",
    "      u'бы', u'и', u'никогда', u'тем', u'был', u'из', u'ним', u'теперь', u'была', u'из-за', \n",
    "      u'них', u'то', u'были', u'или', u'ничего', u'тогда', u'было', u'им', u'но', u'того', \n",
    "      u'быть', u'иногда', u'ну', u'тоже', u'в', u'их', u'о', u'только', u'вам', u'к', u'об', \n",
    "      u'том', u'вас', u'кажется', u'один', u'тот', u'вдруг', u'как', u'он', u'три', u'ведь', \n",
    "      u'какая', u'она', u'тут', u'во', u'какой', u'они', u'ты', u'вот', u'когда', u'опять', \n",
    "      u'у', u'впрочем', u'конечно', u'от', u'уж', u'все', u'которого', u'перед', u'уже', u'всегда', \n",
    "      u'которые', u'по', u'хорошо', u'всего', u'кто', u'под', u'хоть', u'всех', u'куда', u'после',\n",
    "      u'чего', u'всю', u'ли', u'потом', u'человек', u'вы', u'лучше', u'потому', u'чем', u'г', u'между', \n",
    "      u'почти', u'через', u'где', u'меня', u'при', u'что', u'говорил', u'мне', u'про', u'чтоб', u'да', \n",
    "      u'много', u'раз', u'чтобы', u'даже', u'может', u'разве', u'чуть', u'два', u'можно', u'с', u'эти', \n",
    "      u'для', u'мой', u'сам', u'этого', u'до', u'моя', u'свое', u'этой', u'другой', u'мы', u'свою', \n",
    "      u'этом', u'его', u'на', u'себе', u'этот', u'ее', u'над', u'себя', u'эту', u'ей', u'надо', u'сегодня', \n",
    "      u'я', u'ему', u'наконец', u'сейчас', 'если', u'нас', 'есть', u'не', u'также']\n",
    "    \n",
    "stop_words = stop_words + nltk.corpus.stopwords.words('russian')\n",
    "stop_words = list(set(stop_words))\n",
    "\n",
    "punctuation_regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "def get_stop_words_from_tokens(tokens, threshold):\n",
    "    tokens = pd.Series(tokens)\n",
    "    token_frequencies = tokens.value_counts()\n",
    "    return token_frequencies[token_frequencies < threshold].index.values.tolist()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\" Remove single punctuation entry from tokens list \"\"\"\n",
    "    return punctuation_regex.sub('', text) \n",
    "\n",
    "def lemmatized_formatter(text):\n",
    "    return [item.lower().strip() for item in mystem.lemmatize(text) if item.strip() not in [u'', u' ']]\n",
    "\n",
    "def clean_formatter(text):\n",
    "    text = remove_punctuation(text)\n",
    "    lemmas = lemmatized_formatter(text)\n",
    "    tokens = [lemma for lemma in lemmas if not lemma in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = clean_formatter(text)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(articles, formatter):\n",
    "    formatted = [formatter(entry) for entry in articles]\n",
    "    joined = [' '.join(tokens) for tokens in formatted]  \n",
    "    tokenized = [nltk.tokenize.word_tokenize(entry) for entry in joined]    \n",
    "    tokens = [item for sublist in tokenized for item in sublist]\n",
    "    return tokens\n",
    "    \n",
    "def get_ngrams(articles, formatter, ngram):\n",
    "    text_tokens = [formatter(text) for text in articles]\n",
    "    bigrams_generators = [nltk.ngrams(tokens, ngram) for tokens in text_tokens]\n",
    "    bigrams_list = [list(bigrams) for bigrams in bigrams_generators]\n",
    "    bigrams = [item for sublist in bigrams_list for item in sublist]\n",
    "    return bigrams\n",
    "\n",
    "def get_bigrams(articles, formatter):\n",
    "    return get_ngrams(articles, formatter, 2)\n",
    "\n",
    "def get_trigrams(articles, formatter):\n",
    "    return get_ngrams(articles, formatter, 3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# titles = [item['title'] for item in news]\n",
    "# title_tokens = get_tokens(titles, clean_formatter)\n",
    "# title_bigrams = get_bigrams(titles, clean_formatter)\n",
    "# title_trigrams = get_trigrams(titles, clean_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# title_tokens = pd.Series(title_tokens).value_counts()\n",
    "# title_tokens[:20].plot(kind=\"barh\", fontsize=11, alpha=0.9)\n",
    "# plt.savefig('plots/title/most_frequent_words_2.jpeg', format='jpeg', dpi=400, bbox_inches='tight')\n",
    "\n",
    "# title_bigrams = pd.Series(title_bigrams).value_counts()\n",
    "# title_bigrams[:20].plot(kind=\"barh\", fontsize=11, alpha=0.9)\n",
    "# plt.savefig('plots/title/most_frequent_bigrams.jpeg', format='jpeg', dpi=400, bbox_inches='tight')\n",
    "\n",
    "# title_trigrams = pd.Series(title_trigrams).value_counts()\n",
    "# title_trigrams[:20].plot(kind=\"barh\", fontsize=11, alpha=0.9)\n",
    "# plt.savefig('plots/title/most_frequent_trigrams.jpeg', format='jpeg', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# article_tokens = get_tokens(articles, clean_formatter)\n",
    "# article_bigrams = get_bigrams(articles, clean_formatter)\n",
    "# article_trigrams = get_trigrams(articles, clean_formatter)\n",
    "\n",
    "# tokens = pd.Series(article_tokens).value_counts()\n",
    "# tokens[:16].plot(kind=\"barh\", fontsize=11, alpha=0.9)\n",
    "# plt.savefig('plots/most_frequent_words.jpeg', format='jpeg', dpi=400, bbox_inches='tight')\n",
    "\n",
    "# bigrams = pd.Series(article_bigrams).value_counts()\n",
    "# bigrams[1:17].plot(kind=\"barh\", fontsize=11, alpha=0.9)\n",
    "# plt.savefig('plots/most_frequent_bigrams.jpeg', format='jpeg', dpi=400, bbox_inches='tight')\n",
    "\n",
    "# trigrams = pd.Series(article_trigrams).value_counts()\n",
    "# trigrams[:16].plot(kind=\"barh\", fontsize=11, alpha=0.9)\n",
    "# plt.savefig('plots/most_frequent_trigrams.jpeg', format='jpeg', dpi=400, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

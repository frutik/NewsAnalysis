{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local\")\n",
    "         .setAppName(\"News Processing\")\n",
    "         .set(\"spark.executor.memory\", \"9g\"))\n",
    "            \n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import news_config as config\n",
    "from operator import add\n",
    "from pymystem3 import Mystem\n",
    "from NewsMongoMiner import NewsMongoHelper\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mystem = Mystem()\n",
    "mongo_helper = NewsMongoHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news = mongo_helper.get_news(config.pravda_collection)\n",
    "articles = [item['text'] for item in news[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = [u'', u' ', u'-', '\\n', u'–', u'это', u'еще', u'него', u'сказать', u'а', u'ж', u'нее', u'со', u'без', u'же', u'ней', \n",
    "      u'совсем', u'более', u'жизнь', u'нельзя', u'так', u'больше', u'за', u'нет', \n",
    "      u'такой', u'будет', u'зачем', u'ни', u'там', u'будто', u'здесь', u'нибудь', u'тебя', \n",
    "      u'бы', u'и', u'никогда', u'тем', u'был', u'из', u'ним', u'теперь', u'была', u'из-за', \n",
    "      u'них', u'то', u'были', u'или', u'ничего', u'тогда', u'было', u'им', u'но', u'того', \n",
    "      u'быть', u'иногда', u'ну', u'тоже', u'в', u'их', u'о', u'только', u'вам', u'к', u'об', \n",
    "      u'том', u'вас', u'кажется', u'один', u'тот', u'вдруг', u'как', u'он', u'три', u'ведь', \n",
    "      u'какая', u'она', u'тут', u'во', u'какой', u'они', u'ты', u'вот', u'когда', u'опять', \n",
    "      u'у', u'впрочем', u'конечно', u'от', u'уж', u'все', u'которого', u'перед', u'уже', u'всегда', \n",
    "      u'которые', u'по', u'хорошо', u'всего', u'кто', u'под', u'хоть', u'всех', u'куда', u'после',\n",
    "      u'чего', u'всю', u'ли', u'потом', u'человек', u'вы', u'лучше', u'потому', u'чем', u'г', u'между', \n",
    "      u'почти', u'через', u'где', u'меня', u'при', u'что', u'говорил', u'мне', u'про', u'чтоб', u'да', \n",
    "      u'много', u'раз', u'чтобы', u'даже', u'может', u'разве', u'чуть', u'два', u'можно', u'с', u'эти', \n",
    "      u'для', u'мой', u'сам', u'этого', u'до', u'моя', u'свое', u'этой', u'другой', u'мы', u'свою', \n",
    "      u'этом', u'его', u'на', u'себе', u'этот', u'ее', u'над', u'себя', u'эту', u'ей', u'надо', u'сегодня', \n",
    "      u'я', u'ему', u'наконец', u'сейчас', 'если', u'нас', 'есть', u'не', u'также']\n",
    "    \n",
    "stop_words = stop_words + nltk.corpus.stopwords.words('russian')\n",
    "stop_words = list(set(stop_words))\n",
    "\n",
    "punctuation_regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "def get_stop_words_from_tokens(tokens, threshold):\n",
    "    tokens = pd.Series(tokens)\n",
    "    token_frequencies = tokens.value_counts()\n",
    "    return token_frequencies[token_frequencies < threshold].index.values.tolist()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\" Remove single punctuation entry from tokens list \"\"\"\n",
    "    return punctuation_regex.sub('', text) \n",
    "\n",
    "def lemmatized_formatter(text):\n",
    "    return [item.lower().strip() for item in mystem.lemmatize(text) if item.strip() not in [u'', u' ']]\n",
    "\n",
    "def clean_formatter(text):\n",
    "    text = remove_punctuation(text)\n",
    "    lemmas = lemmatized_formatter(text)\n",
    "    tokens = [lemma for lemma in lemmas if not lemma in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def clean_text(text):\n",
    "    if(not text):\n",
    "        return ''\n",
    "    tokens = clean_formatter(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "class TextHelper:\n",
    "    @staticmethod\n",
    "    def get_tokens(text):\n",
    "        return nltk.tokenize.word_tokenize(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_ngrams(text, ngrams):\n",
    "        tokens = TextHelper.get_tokens(text)\n",
    "        return list(nltk.ngrams(tokens, ngrams))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_bigrams(text):\n",
    "        return TextHelper.get_ngrams(text, 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_trigrams(text):\n",
    "        return TextHelper.get_ngrams(text, 3)   \n",
    "    \n",
    "class CorporaHelper:\n",
    "    @staticmethod\n",
    "    def get_tokens(articles):\n",
    "        tokenized = [nltk.tokenize.word_tokenize(article) for article in articles]    \n",
    "        tokens = [item for sublist in tokenized for item in sublist]\n",
    "        return tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_ngrams(articles, ngrams):\n",
    "        text_tokens = [nltk.word_tokenize(article) for article in articles]\n",
    "        bigrams_generators = [nltk.ngrams(tokens, ngrams) for tokens in text_tokens]\n",
    "        bigrams_list = [list(bigrams) for bigrams in bigrams_generators]\n",
    "        bigrams = [item for sublist in bigrams_list for item in sublist]\n",
    "        return bigrams\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_bigrams(articles):\n",
    "        return CorporaHelper.get_ngrams(articles, 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_trigrams(articles):\n",
    "        return CorporaHelper.get_ngrams(articles, 3)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic cleaning of news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_news(entry):\n",
    "    entry['title'] = clean_text(entry['title'])\n",
    "    entry['text'] = clean_text(entry['text'])\n",
    "    entry['summary'] = clean_text(entry['summary'])\n",
    "    return entry\n",
    "\n",
    "news_cleaned = (sc.parallelize(news)\n",
    "                .map(clean_news))\n",
    "\n",
    "news_cleaned.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pymongo import MongoClient\n",
    "# mongo_client = MongoClient(config.mongo_db_connection_string)\n",
    "# collection = mongo_client[config.db]['pravda_cleaned']\n",
    "# collection.insert(news_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and searching for stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news = mongo_helper.get_news(config.pravda_cleaned_collection)\n",
    "articles = [entry[\"text\"] for entry in news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_count(words_rdd):\n",
    "    return words_rdd.map(lambda word: (word,1)).reduceByKey(add)\n",
    "\n",
    "news_rdd = sc.parallelize(news)\n",
    "articles_rdd = sc.parallelize(articles)\n",
    "tokens = articles_rdd.flatMap(TextHelper.get_tokens)\n",
    "tokens_count = word_count(tokens)\n",
    "tokens_count.cache()\n",
    "\n",
    "most_frequent = tokens_count.takeOrdered(15, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokens_statistics(tokens_count):\n",
    "    all_words_count = tokens_count.map(lambda (k,v): v).reduce(add)\n",
    "    all_unique_words_count = tokens_count.map(lambda (k,v): k).count()\n",
    "    rare_words_count = tokens_count.filter(lambda (k,v): v == 1).map(lambda (k,v): v).count()\n",
    "    rare_unique_words_count = tokens_count.filter(lambda (k,v): v == 1).map(lambda (k,v): k).count()\n",
    "    print 'Total number of words', all_words_count\n",
    "    print 'Total number of unique words', all_unique_words_count\n",
    "    print 'Total number of rare words', rare_words_count\n",
    "    print 'Number of unique rare words', rare_unique_words_count\n",
    "    \n",
    "tokens_statistics(tokens_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rare_tokens_rdd = tokens_count.filter(lambda (k,v): v == 1) #.map(lambda (word, count): word)\n",
    "\n",
    "#flattens news to (token, (_id, position)) represantation and removes rare tokens from rdd\n",
    "used_tokens = (news_rdd\n",
    " .map(lambda item: (item['_id'], item['text']))\n",
    " .mapValues(TextHelper.get_tokens)\n",
    " .mapValues(lambda tokens: [(token, position) for position, token in enumerate(tokens)])\n",
    " .map(lambda (_id, tokens_with_positions): [(token, (_id, position)) for (token, position) in tokens_with_positions])\n",
    " .flatMap(lambda item: item)\n",
    " .subtractByKey(rare_tokens_rdd))\n",
    "\n",
    "#convers (token, (_id, position)) to (_id, text) form\n",
    "texts_without_rare_words = (used_tokens\n",
    " .map(lambda (token, (_id, position)): (_id, (position, token)))\n",
    " .groupByKey()\n",
    " .mapValues(lambda items: [token for (position, token) in sorted(items)])\n",
    " .mapValues(lambda tokens: u' '.join(tokens)))\n",
    "\n",
    "news_without_text = (news_rdd\n",
    "                     .map(lambda article: {key:value for key, value in article.items() if key not in [u'text']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles_rdd = sc.parallelize(articles)\n",
    "articles_clean = articles_rdd.map(clean_text)\n",
    "articles_clean.cache()\n",
    "article_tokens = articles_clean.map(TextHelper.get_tokens)\n",
    "tokens = article_tokens.flatMap(lambda token: token).map(lambda token: (token, 1))\n",
    "tokens.cache()\n",
    "tokens_frequency = tokens.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequent = tokens_frequency.takeOrdered(15, key = lambda x: -x[1])\n",
    "for item in frequent[:10]:\n",
    "    print item[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
